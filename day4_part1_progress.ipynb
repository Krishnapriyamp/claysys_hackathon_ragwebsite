{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DZLKmg0bdp-"
      },
      "outputs": [],
      "source": [
        "!pip install -U google-genai faiss-cpu beautifulsoup4 requests numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import faiss\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google import genai\n",
        "\n",
        "# Set your Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key\"\n",
        "\n",
        "client = genai.Client()"
      ],
      "metadata": {
        "id": "oRyJ3wUcd0fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_website(url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Remove unwanted elements\n",
        "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\"]):\n",
        "        tag.decompose()\n",
        "\n",
        "    paragraphs = soup.find_all(\"p\")\n",
        "    text = \" \".join([p.get_text() for p in paragraphs])\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "gV3oQpK_b27A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=400, overlap=80):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(words):\n",
        "        end = start + chunk_size\n",
        "        chunk = \" \".join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "V2I_VUbzeCsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
        "\n",
        "website_text = load_website(url)\n",
        "chunks = chunk_text(website_text)\n",
        "\n",
        "documents = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    documents.append({\n",
        "        \"text\": chunk,\n",
        "        \"source\": f\"{url} | Chunk {i+1}\"\n",
        "    })\n",
        "\n",
        "print(\"Total Chunks:\", len(documents))"
      ],
      "metadata": {
        "id": "tRLYw3kveGgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_text(text):\n",
        "    response = client.models.embed_content(\n",
        "        model=\"models/gemini-embedding-001\",\n",
        "        contents=text\n",
        "    )\n",
        "    return np.array(response.embeddings[0].values, dtype=\"float32\")"
      ],
      "metadata": {
        "id": "ozYGlKuTeK5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = [embed_text(doc[\"text\"]) for doc in documents]\n",
        "dimension = embeddings[0].shape[0]\n",
        "\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(embeddings))\n",
        "\n",
        "print(\"FAISS index built successfully.\")"
      ],
      "metadata": {
        "id": "JteGZ3jYeN4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, top_k=8):\n",
        "    query_vector = embed_text(query)\n",
        "    distances, indices = index.search(np.array([query_vector]), top_k)\n",
        "\n",
        "    results = []\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        results.append({\n",
        "            \"text\": documents[idx][\"text\"],\n",
        "            \"source\": documents[idx][\"source\"],\n",
        "            \"score\": float(dist)\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "He8vQt5EeUfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SIMILARITY_THRESHOLD = 1.2\n",
        "\n",
        "def guardrail_filter(results):\n",
        "    return [r for r in results if r[\"score\"] < SIMILARITY_THRESHOLD]"
      ],
      "metadata": {
        "id": "Vi8cjyb0eXiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rerank(query, retrieved_chunks):\n",
        "    chunk_texts = \"\\n\\n\".join(\n",
        "        [f\"Chunk {i+1}: {chunk['text']}\" for i, chunk in enumerate(retrieved_chunks)]\n",
        "    )\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Query: {query}\n",
        "\n",
        "    Rank the top 3 most relevant chunks by number.\n",
        "\n",
        "    {chunk_texts}\n",
        "\n",
        "    Return only numbers separated by commas.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"models/gemini-2.5-flash\",\n",
        "        contents=prompt\n",
        "    )\n",
        "\n",
        "    ranked_numbers = response.text.strip()\n",
        "    top_indices = [int(n.strip()) - 1 for n in ranked_numbers.split(\",\") if n.strip().isdigit()]\n",
        "\n",
        "    return [retrieved_chunks[i] for i in top_indices if i < len(retrieved_chunks)]"
      ],
      "metadata": {
        "id": "3ByD-raieaiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query):\n",
        "    retrieved = retrieve(query)\n",
        "    filtered = guardrail_filter(retrieved)\n",
        "\n",
        "    if not filtered:\n",
        "        return \"The answer is not available in the provided website content.\", []\n",
        "\n",
        "    reranked = rerank(query, filtered)\n",
        "\n",
        "    context = \"\\n\\n\".join([chunk[\"text\"] for chunk in reranked])\n",
        "    sources = list(set([chunk[\"source\"] for chunk in reranked]))\n",
        "\n",
        "    final_prompt = f\"\"\"\n",
        "    Answer the question strictly using the context below.\n",
        "    If the answer is not found, say it is not available.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {query}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"models/gemini-2.5-flash\",\n",
        "        contents=final_prompt\n",
        "    )\n",
        "\n",
        "    return response.text, sources"
      ],
      "metadata": {
        "id": "0I1Efl8ted-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between AI and Machine Learning?\"\n",
        "\n",
        "answer, sources = generate_answer(query)\n",
        "\n",
        "print(\"Answer:\\n\")\n",
        "print(answer)\n",
        "\n",
        "print(\"\\nSources:\")\n",
        "for s in sources:\n",
        "    print(\"-\", s)"
      ],
      "metadata": {
        "id": "a2JhrveGehmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Cosine Similarity Re-ranking (Upgrade 1)\n",
        "\n",
        "def cosine_rerank(query, retrieved_chunks, top_k=3):\n",
        "    query_vec = embed_text(query)\n",
        "\n",
        "    reranked = []\n",
        "\n",
        "    for chunk in retrieved_chunks:\n",
        "        chunk_vec = embed_text(chunk[\"text\"])\n",
        "\n",
        "        cosine_sim = np.dot(query_vec, chunk_vec) / (\n",
        "            np.linalg.norm(query_vec) * np.linalg.norm(chunk_vec)\n",
        "        )\n",
        "\n",
        "        chunk[\"cosine_score\"] = float(cosine_sim)\n",
        "        reranked.append(chunk)\n",
        "\n",
        "    reranked = sorted(reranked, key=lambda x: x[\"cosine_score\"], reverse=True)\n",
        "\n",
        "    return reranked[:top_k]"
      ],
      "metadata": {
        "id": "kHqewe85kaXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sources(chunks):\n",
        "    return list(set(chunk[\"source\"] for chunk in chunks))"
      ],
      "metadata": {
        "id": "vmSHPWExkdA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Confidence Estimation (Upgrade 3)\n",
        "\n",
        "def calculate_confidence(reranked_chunks):\n",
        "    if not reranked_chunks:\n",
        "        return 0.0\n",
        "\n",
        "    avg_score = np.mean([chunk[\"cosine_score\"] for chunk in reranked_chunks])\n",
        "\n",
        "    # Normalize between 0 and 1 (cosine is already between -1 and 1)\n",
        "    confidence = max(0, min(1, (avg_score + 1) / 2))\n",
        "\n",
        "    return round(confidence, 3)"
      ],
      "metadata": {
        "id": "LdsP0QxNLrp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query):\n",
        "\n",
        "    # Step 1: Retrieve\n",
        "    retrieved = retrieve(query)\n",
        "\n",
        "    # Step 2: Guardrail Filter\n",
        "    filtered = guardrail_filter(retrieved)\n",
        "\n",
        "    if not filtered:\n",
        "        return {\n",
        "            \"answer\": \"The answer is not available in the provided website content.\",\n",
        "            \"sources\": [],\n",
        "            \"confidence\": 0.0\n",
        "        }\n",
        "\n",
        "    # Step 3: Cosine Re-rank\n",
        "    reranked = cosine_rerank(query, filtered)\n",
        "\n",
        "    # Step 4: Prepare Context\n",
        "    context = \"\\n\\n\".join([chunk[\"text\"] for chunk in reranked])\n",
        "\n",
        "    # Step 5: Source Attribution\n",
        "    sources = extract_sources(reranked)\n",
        "\n",
        "    # Step 6: Final Answer Generation\n",
        "    final_prompt = f\"\"\"\n",
        "    Answer the question strictly using the context below.\n",
        "    If the answer is not found, say it is not available.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {query}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"models/gemini-2.5-flash\",\n",
        "        contents=final_prompt\n",
        "    )\n",
        "\n",
        "    # Step 7: Confidence\n",
        "    confidence = calculate_confidence(reranked)\n",
        "\n",
        "    return {\n",
        "        \"answer\": response.text,\n",
        "        \"sources\": sources,\n",
        "        \"confidence\": confidence\n",
        "    }"
      ],
      "metadata": {
        "id": "Ib79ysZjLvgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between AI and Machine Learning?\"\n",
        "\n",
        "result = generate_answer(query)\n",
        "\n",
        "print(\"Answer:\\n\")\n",
        "print(result[\"answer\"])\n",
        "\n",
        "print(\"\\nSources:\")\n",
        "for s in result[\"sources\"]:\n",
        "    print(\"-\", s)\n",
        "\n",
        "print(\"\\nConfidence Score:\", result[\"confidence\"])"
      ],
      "metadata": {
        "id": "mNRyO7AyLyu_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}