{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key\""
      ],
      "metadata": {
        "id": "2ik9hsAeYKxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "print(\"API configured successfully\")"
      ],
      "metadata": {
        "id": "SLJCDeEVYfkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-genai"
      ],
      "metadata": {
        "id": "itfJDEQEY1g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "import os\n",
        "\n",
        "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Explain machine learning in simple words\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "kG8FAuExZvmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in client.models.list():\n",
        "    print(m.name)"
      ],
      "metadata": {
        "id": "c5rk_tFR8Ku8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OffdL3CO_jq"
      },
      "outputs": [],
      "source": [
        "#Install Libraries\n",
        "!pip install requests beautifulsoup4 sentence-transformers faiss-cpu openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vTfdwd77PNiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step1::Web Scraping\n",
        "url = \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Extract only main content area\n",
        "content = soup.find(\"div\", {\"id\": \"mw-content-text\"})\n",
        "\n",
        "# Remove unwanted tags\n",
        "for script in content([\"script\", \"style\", \"sup\", \"table\"]):\n",
        "    script.decompose()\n",
        "\n",
        "text = content.get_text(separator=\" \")\n",
        "\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "oOqgAM3cPVZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step2::text into chunks\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "        chunks.append(text[i:i+chunk_size])\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(text)\n",
        "\n",
        "print(\"Total chunks:\", len(chunks))\n",
        "print(\"First chunk preview:\\n\", chunks[0])"
      ],
      "metadata": {
        "id": "dpEh2F3-RDB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step3::Generate Embeddings\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Convert chunks into embeddings\n",
        "embeddings = model.encode(chunks)\n",
        "\n",
        "print(\"Embedding shape:\", embeddings.shape)"
      ],
      "metadata": {
        "id": "9iHbmBKpRqBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step4::Create FAISS Vector Database\n",
        "dimension = embeddings.shape[1]\n",
        "\n",
        "# Create FAISS index\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Add embeddings to index\n",
        "index.add(np.array(embeddings))\n",
        "\n",
        "print(\"Total vectors stored in FAISS:\", index.ntotal)"
      ],
      "metadata": {
        "id": "Sb5dc6bKSD2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 5::Ask Question & Retrieve\n",
        "question = \"What is machine learning?\"\n",
        "\n",
        "# Convert question to embedding\n",
        "question_embedding = model.encode([question])\n",
        "\n",
        "# Retrieve top 3 similar chunks\n",
        "k = 3\n",
        "distances, indices = index.search(np.array(question_embedding), k)\n",
        "\n",
        "retrieved_chunks = [chunks[i] for i in indices[0]]\n",
        "\n",
        "print(\"Retrieved Chunks:\\n\")\n",
        "\n",
        "for i, chunk in enumerate(retrieved_chunks):\n",
        "    print(f\"Chunk {i+1}:\\n\")\n",
        "    print(chunk[:500])\n",
        "    print(\"\\n--------------------\\n\")"
      ],
      "metadata": {
        "id": "h82_JKAISOWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "id": "b4MaJi--YJMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(question)"
      ],
      "metadata": {
        "id": "1LeQkZW4aiTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 6::Connect to OpenAI\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Configure API\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# Load model\n",
        "model_llm = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "# Combine retrieved chunks\n",
        "context = \" \".join(retrieved_chunks)\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Answer the question using ONLY the context below.\n",
        "If the answer is not in the context, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "response = model_llm.generate_content(prompt)\n",
        "\n",
        "print(\"Final Answer:\\n\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "sxvmUjAJaehz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sources:\")\n",
        "for i, chunk in enumerate(retrieved_chunks):\n",
        "    print(f\"Source chunk {i+1}\")"
      ],
      "metadata": {
        "id": "g4648q7JbFt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Day 2:: full Retrieval-Augmented Generation (RAG) pipeline"
      ],
      "metadata": {
        "id": "XdHpb_d79DuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step1::install Required Dependencies\n",
        "!pip install -q langchain langchain-community langchain-text-splitters faiss-cpu pypdf google-generativeai"
      ],
      "metadata": {
        "id": "UT_ytOce-iYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "LYlWWOIe_SsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key\"\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "print(\"API configured successfully\")"
      ],
      "metadata": {
        "id": "eM9G9qUh_XQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step2:: Load PDF\n",
        "loader = PyPDFLoader(\"/content/sample.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "print(\"Number of pages loaded:\", len(documents))"
      ],
      "metadata": {
        "id": "om6T7FTL_euh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step3:: Split Document into Text Chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(\"Number of chunks created:\", len(chunks))"
      ],
      "metadata": {
        "id": "a23Jpg2RBGEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step4:: Install  Google GenAI SDK\n",
        "!pip install -q google-genai"
      ],
      "metadata": {
        "id": "IDy4tiJdByhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5:: Configure Gemini API Client and Embedding Model\n",
        "from google import genai\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Set API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key\"\n",
        "\n",
        "# Create client\n",
        "client = genai.Client()\n",
        "\n",
        "# Correct embedding model from your list\n",
        "embedding_model = \"models/gemini-embedding-001\"\n",
        "\n",
        "def get_embedding(text):\n",
        "    response = client.models.embed_content(\n",
        "        model=embedding_model,\n",
        "        contents=text\n",
        "    )\n",
        "    return response.embeddings[0].values"
      ],
      "metadata": {
        "id": "XdYuwlIXB3f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6:: Generate Embeddings for All Document Chunks\n",
        "chunk_embeddings = []\n",
        "\n",
        "for chunk in chunks:\n",
        "    emb = get_embedding(chunk.page_content)\n",
        "    chunk_embeddings.append(emb)\n",
        "\n",
        "chunk_embeddings = np.array(chunk_embeddings).astype(\"float32\")\n",
        "\n",
        "print(\"Embedding shape:\", chunk_embeddings.shape)"
      ],
      "metadata": {
        "id": "Ldz0afroB-id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7:: Create FAISS Vector Index for Semantic Search\n",
        "import faiss\n",
        "\n",
        "dimension = chunk_embeddings.shape[1]\n",
        "\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(chunk_embeddings)\n",
        "\n",
        "print(\"Total vectors stored in FAISS:\", index.ntotal)"
      ],
      "metadata": {
        "id": "HztM9ZXL8fBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 8:: Implement Semantic Retrieval Function\n",
        "def retrieve(query, k=3):\n",
        "    # Create query embedding\n",
        "    query_embedding = client.models.embed_content(\n",
        "        model=embedding_model,\n",
        "        contents=query\n",
        "    ).embeddings[0].values\n",
        "\n",
        "    query_embedding = np.array([query_embedding]).astype(\"float32\")\n",
        "\n",
        "    # Search in FAISS\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    print(\"Retrieved chunk indices:\", indices)\n",
        "\n",
        "    retrieved_texts = [chunks[i].page_content for i in indices[0]]\n",
        "\n",
        "    return retrieved_texts"
      ],
      "metadata": {
        "id": "uzxLw__f8jY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = retrieve(\"What is the objective of the proposed model?\")"
      ],
      "metadata": {
        "id": "wNTxWpGA84tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9:: Implement Retrieval-Augmented Answer Generation\n",
        "def generate_answer(query):\n",
        "    retrieved_chunks = retrieve(query)\n",
        "\n",
        "    context = \"\\n\\n\".join(retrieved_chunks)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Answer the question using ONLY the context below.\n",
        "If the answer is not in the context, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"models/gemini-2.5-flash\",\n",
        "        contents=prompt\n",
        "    )\n",
        "\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "zxCHFp489A98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10:: Execute Full RAG Pipeline with User Question\n",
        "question = \"What are the future works suggested?\"\n",
        "\n",
        "answer = generate_answer(question)\n",
        "\n",
        "print(\"Final Answer:\\n\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "MzEJCYqJ9M2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# day3 progress::structured and unstructed data handling"
      ],
      "metadata": {
        "id": "MISuJVRvia_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# step1::Create sample structured dataset\n",
        "data = {\n",
        "    \"Model\": [\"CNN-A\", \"CNN-B\", \"CNN-C\"],\n",
        "    \"Accuracy\": [91.2, 94.8, 88.5],\n",
        "    \"Dataset\": [\"HAM10000\", \"ISIC\", \"HAM10000\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(\"structured_data.csv\", index=False)\n",
        "\n",
        "print(\"Sample CSV created successfully!\")\n",
        "df"
      ],
      "metadata": {
        "id": "EWWFN7OIjyzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step2::Convert Rows to Text Format\n",
        "structured_texts = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    row_text = \" | \".join([f\"{col}: {row[col]}\" for col in df.columns])\n",
        "    structured_texts.append(row_text)\n",
        "\n",
        "print(\"Example structured text:\")\n",
        "print(structured_texts[0])"
      ],
      "metadata": {
        "id": "-q6mz8SHih3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step3::Generate Structured Embeddings\n",
        "structured_embeddings = []\n",
        "\n",
        "for text in structured_texts:\n",
        "    emb = get_embedding(text)\n",
        "    structured_embeddings.append(emb)\n",
        "\n",
        "structured_embeddings = np.array(structured_embeddings).astype(\"float32\")\n",
        "\n",
        "print(\"Structured Embedding Shape:\", structured_embeddings.shape)"
      ],
      "metadata": {
        "id": "G66CvbMfim5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step4::Create FAISS Index for Structured Data\n",
        "import faiss\n",
        "\n",
        "structured_dimension = structured_embeddings.shape[1]\n",
        "\n",
        "structured_index = faiss.IndexFlatL2(structured_dimension)\n",
        "structured_index.add(structured_embeddings)\n",
        "\n",
        "print(\"Structured vectors stored:\", structured_index.ntotal)"
      ],
      "metadata": {
        "id": "ebXPhyn5kvXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step5::Query Type Detection\n",
        "def detect_query_type(query):\n",
        "\n",
        "    structured_keywords = [\n",
        "        \"accuracy\",\n",
        "        \"percentage\",\n",
        "        \"highest\",\n",
        "        \"lowest\",\n",
        "        \"greater than\",\n",
        "        \"less than\",\n",
        "        \"maximum\",\n",
        "        \"minimum\"\n",
        "    ]\n",
        "\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    for word in structured_keywords:\n",
        "        if word in query_lower:\n",
        "            return \"structured\"\n",
        "\n",
        "    return \"unstructured\""
      ],
      "metadata": {
        "id": "dnD-FkRJkzHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def structured_answer(query):\n",
        "\n",
        "    query = query.lower()\n",
        "\n",
        "    # Example for model accuracy dataset\n",
        "    if \"highest\" in query and \"accuracy\" in query:\n",
        "        max_row = df.loc[df[\"Accuracy\"].idxmax()]\n",
        "        return f\"{max_row['Model']} has the highest accuracy of {max_row['Accuracy']}%\"\n",
        "\n",
        "    if \"lowest\" in query and \"accuracy\" in query:\n",
        "        min_row = df.loc[df[\"Accuracy\"].idxmin()]\n",
        "        return f\"{min_row['Model']} has the lowest accuracy of {min_row['Accuracy']}%\"\n",
        "\n",
        "    return \"I don't know.\""
      ],
      "metadata": {
        "id": "xEP_4D99qli6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step6::Hybrid Retrieve Function\n",
        "def hybrid_retrieve(query, k=3):\n",
        "\n",
        "    query_type = detect_query_type(query)\n",
        "\n",
        "    if query_type == \"structured\":\n",
        "        print(\"Query Type: Structured\")\n",
        "        # Directly return structured answer instead of retrieval\n",
        "        return [structured_answer(query)]\n",
        "\n",
        "    else:\n",
        "        print(\"Query Type: Unstructured\")\n",
        "\n",
        "        query_embedding = client.models.embed_content(\n",
        "            model=embedding_model,\n",
        "            contents=query\n",
        "        ).embeddings[0].values\n",
        "\n",
        "        query_embedding = np.array([query_embedding]).astype(\"float32\")\n",
        "\n",
        "        distances, indices = index.search(query_embedding, k)\n",
        "        retrieved = [chunks[i].page_content for i in indices[0]]\n",
        "\n",
        "        return retrieved"
      ],
      "metadata": {
        "id": "ergXOmXok3By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step7::Generate Final Answer\n",
        "def generate_hybrid_answer(query):\n",
        "\n",
        "    retrieved_chunks = hybrid_retrieve(query)\n",
        "\n",
        "    context = \"\\n\\n\".join(retrieved_chunks)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Answer the question using ONLY the context below.\n",
        "If numerical values are present, use exact values.\n",
        "If answer is not in context, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"models/gemini-2.5-flash\",\n",
        "        contents=prompt\n",
        "    )\n",
        "\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "nefjQHYfl0xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step8::Test Structured Query\n",
        "question = \"Which model has highest accuracy?\"\n",
        "\n",
        "answer = generate_hybrid_answer(question)\n",
        "\n",
        "print(\"Final Answer:\\n\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "wHA5uwWwk9fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step9::Test Unstructured Query\n",
        "question = \"What is the objective of the proposed model?\"\n",
        "\n",
        "answer = generate_hybrid_answer(question)\n",
        "\n",
        "print(\"Final Answer:\\n\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "Z2flmxyAmHhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "metadata": {
        "id": "QkeOeK9YqZZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip amazon.csv.zip"
      ],
      "metadata": {
        "id": "1l4LbputqinU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"amazon.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "avixMdGHqmoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------------- LOAD CSV ----------------\n",
        "df = pd.read_csv(\"amazon.csv\")\n",
        "\n",
        "print(\"CSV Loaded Successfully\")\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "\n",
        "# ---------------- CLEAN NUMERIC COLUMNS ----------------\n",
        "def clean_numeric_column(col):\n",
        "    return pd.to_numeric(\n",
        "        df[col].astype(str)\n",
        "        .str.replace(\"â‚¹\", \"\", regex=False)\n",
        "        .str.replace(\",\", \"\", regex=False)\n",
        "        .str.replace(\"%\", \"\", regex=False),\n",
        "        errors=\"coerce\"\n",
        "    )\n",
        "\n",
        "possible_numeric_cols = [\n",
        "    \"discounted_price\",\n",
        "    \"actual_price\",\n",
        "    \"discount_percentage\",\n",
        "    \"rating\",\n",
        "    \"rating_count\"\n",
        "]\n",
        "\n",
        "for col in possible_numeric_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = clean_numeric_column(col)\n",
        "\n",
        "print(\"\\nNumeric conversion done.\")\n",
        "print(df.dtypes)\n",
        "\n",
        "\n",
        "# ---------------- QUERY TYPE DETECTION ----------------\n",
        "def detect_query_type(query):\n",
        "    structured_keywords = [\n",
        "        \"highest\", \"lowest\", \"maximum\", \"minimum\",\n",
        "        \"greater than\", \"less than\", \"average\",\n",
        "        \"price\", \"rating\", \"count\"\n",
        "    ]\n",
        "\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    for word in structured_keywords:\n",
        "        if word in query_lower:\n",
        "            return \"structured\"\n",
        "\n",
        "    return \"unstructured\"\n",
        "\n",
        "\n",
        "# ---------------- STRUCTURED ANSWER LOGIC ----------------\n",
        "def structured_answer(query):\n",
        "\n",
        "    query = query.lower()\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
        "\n",
        "    if len(numeric_cols) == 0:\n",
        "        return \"No numeric columns found in dataset.\"\n",
        "\n",
        "    # Map simple words to actual column names\n",
        "    column_map = {\n",
        "        \"price\": \"discounted_price\",\n",
        "        \"actual price\": \"actual_price\",\n",
        "        \"discount\": \"discount_percentage\",\n",
        "        \"rating\": \"rating\",\n",
        "        \"count\": \"rating_count\"\n",
        "    }\n",
        "\n",
        "    selected_column = None\n",
        "\n",
        "    for key in column_map:\n",
        "        if key in query and column_map[key] in numeric_cols:\n",
        "            selected_column = column_map[key]\n",
        "            break\n",
        "\n",
        "    if selected_column is None:\n",
        "        return \"I don't know.\"\n",
        "\n",
        "    # Highest\n",
        "    if \"highest\" in query or \"maximum\" in query:\n",
        "        row = df.loc[df[selected_column].idxmax()]\n",
        "        return f\"{row['product_name']} has the highest {selected_column} of {row[selected_column]}\"\n",
        "\n",
        "    # Lowest\n",
        "    if \"lowest\" in query or \"minimum\" in query:\n",
        "        row = df.loc[df[selected_column].idxmin()]\n",
        "        return f\"{row['product_name']} has the lowest {selected_column} of {row[selected_column]}\"\n",
        "\n",
        "    # Average\n",
        "    if \"average\" in query:\n",
        "        avg = df[selected_column].mean()\n",
        "        return f\"The average {selected_column} is {round(avg, 2)}\"\n",
        "\n",
        "    return \"I don't know.\"\n",
        "\n",
        "\n",
        "# ---------------- HYBRID ANSWER ----------------\n",
        "def generate_csv_rag_answer(query):\n",
        "\n",
        "    qtype = detect_query_type(query)\n",
        "    print(\"Query Type:\", qtype)\n",
        "\n",
        "    if qtype == \"structured\":\n",
        "        return structured_answer(query)\n",
        "    else:\n",
        "        return \"Unstructured queries not supported in this CSV-only demo.\"\n",
        "\n",
        "\n",
        "# ---------------- TEST ----------------\n",
        "question = \"Which product has highest price?\"\n",
        "answer = generate_csv_rag_answer(question)\n",
        "\n",
        "print(\"\\nFinal Answer:\\n\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "K4Sk_Nqbq8az"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}