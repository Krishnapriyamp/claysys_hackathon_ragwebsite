{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key\""
      ],
      "metadata": {
        "id": "2ik9hsAeYKxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "print(\"API configured successfully\")"
      ],
      "metadata": {
        "id": "SLJCDeEVYfkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-genai"
      ],
      "metadata": {
        "id": "itfJDEQEY1g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "import os\n",
        "\n",
        "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Explain machine learning in simple words\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "kG8FAuExZvmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in client.models.list():\n",
        "    print(m.name)"
      ],
      "metadata": {
        "id": "c5rk_tFR8Ku8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OffdL3CO_jq"
      },
      "outputs": [],
      "source": [
        "#Install Libraries\n",
        "!pip install requests beautifulsoup4 sentence-transformers faiss-cpu openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vTfdwd77PNiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step1::Web Scraping\n",
        "url = \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Extract only main content area\n",
        "content = soup.find(\"div\", {\"id\": \"mw-content-text\"})\n",
        "\n",
        "# Remove unwanted tags\n",
        "for script in content([\"script\", \"style\", \"sup\", \"table\"]):\n",
        "    script.decompose()\n",
        "\n",
        "text = content.get_text(separator=\" \")\n",
        "\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "oOqgAM3cPVZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step2::text into chunks\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "        chunks.append(text[i:i+chunk_size])\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(text)\n",
        "\n",
        "print(\"Total chunks:\", len(chunks))\n",
        "print(\"First chunk preview:\\n\", chunks[0])"
      ],
      "metadata": {
        "id": "dpEh2F3-RDB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step3::Generate Embeddings\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Convert chunks into embeddings\n",
        "embeddings = model.encode(chunks)\n",
        "\n",
        "print(\"Embedding shape:\", embeddings.shape)"
      ],
      "metadata": {
        "id": "9iHbmBKpRqBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step4::Create FAISS Vector Database\n",
        "dimension = embeddings.shape[1]\n",
        "\n",
        "# Create FAISS index\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Add embeddings to index\n",
        "index.add(np.array(embeddings))\n",
        "\n",
        "print(\"Total vectors stored in FAISS:\", index.ntotal)"
      ],
      "metadata": {
        "id": "Sb5dc6bKSD2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 5::Ask Question & Retrieve\n",
        "question = \"What is machine learning?\"\n",
        "\n",
        "# Convert question to embedding\n",
        "question_embedding = model.encode([question])\n",
        "\n",
        "# Retrieve top 3 similar chunks\n",
        "k = 3\n",
        "distances, indices = index.search(np.array(question_embedding), k)\n",
        "\n",
        "retrieved_chunks = [chunks[i] for i in indices[0]]\n",
        "\n",
        "print(\"Retrieved Chunks:\\n\")\n",
        "\n",
        "for i, chunk in enumerate(retrieved_chunks):\n",
        "    print(f\"Chunk {i+1}:\\n\")\n",
        "    print(chunk[:500])\n",
        "    print(\"\\n--------------------\\n\")"
      ],
      "metadata": {
        "id": "h82_JKAISOWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "id": "b4MaJi--YJMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(question)"
      ],
      "metadata": {
        "id": "1LeQkZW4aiTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 6::Connect to OpenAI\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Configure API\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# Load model\n",
        "model_llm = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "# Combine retrieved chunks\n",
        "context = \" \".join(retrieved_chunks)\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Answer the question using ONLY the context below.\n",
        "If the answer is not in the context, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "response = model_llm.generate_content(prompt)\n",
        "\n",
        "print(\"Final Answer:\\n\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "sxvmUjAJaehz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sources:\")\n",
        "for i, chunk in enumerate(retrieved_chunks):\n",
        "    print(f\"Source chunk {i+1}\")"
      ],
      "metadata": {
        "id": "g4648q7JbFt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Day 2:: full Retrieval-Augmented Generation (RAG) pipeline"
      ],
      "metadata": {
        "id": "XdHpb_d79DuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step1::install Required Dependencies\n",
        "!pip install -q langchain langchain-community langchain-text-splitters faiss-cpu pypdf google-generativeai"
      ],
      "metadata": {
        "id": "UT_ytOce-iYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "LYlWWOIe_SsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key\"\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "print(\"API configured successfully\")"
      ],
      "metadata": {
        "id": "eM9G9qUh_XQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step2:: Load PDF\n",
        "loader = PyPDFLoader(\"/content/sample.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "print(\"Number of pages loaded:\", len(documents))"
      ],
      "metadata": {
        "id": "om6T7FTL_euh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step3:: Split Document into Text Chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(\"Number of chunks created:\", len(chunks))"
      ],
      "metadata": {
        "id": "a23Jpg2RBGEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step4:: Install  Google GenAI SDK\n",
        "!pip install -q google-genai"
      ],
      "metadata": {
        "id": "IDy4tiJdByhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5:: Configure Gemini API Client and Embedding Model\n",
        "from google import genai\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Set API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key\"\n",
        "\n",
        "# Create client\n",
        "client = genai.Client()\n",
        "\n",
        "# Correct embedding model from your list\n",
        "embedding_model = \"models/gemini-embedding-001\"\n",
        "\n",
        "def get_embedding(text):\n",
        "    response = client.models.embed_content(\n",
        "        model=embedding_model,\n",
        "        contents=text\n",
        "    )\n",
        "    return response.embeddings[0].values"
      ],
      "metadata": {
        "id": "XdYuwlIXB3f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6:: Generate Embeddings for All Document Chunks\n",
        "chunk_embeddings = []\n",
        "\n",
        "for chunk in chunks:\n",
        "    emb = get_embedding(chunk.page_content)\n",
        "    chunk_embeddings.append(emb)\n",
        "\n",
        "chunk_embeddings = np.array(chunk_embeddings).astype(\"float32\")\n",
        "\n",
        "print(\"Embedding shape:\", chunk_embeddings.shape)"
      ],
      "metadata": {
        "id": "Ldz0afroB-id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7:: Create FAISS Vector Index for Semantic Search\n",
        "import faiss\n",
        "\n",
        "dimension = chunk_embeddings.shape[1]\n",
        "\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(chunk_embeddings)\n",
        "\n",
        "print(\"Total vectors stored in FAISS:\", index.ntotal)"
      ],
      "metadata": {
        "id": "HztM9ZXL8fBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 8:: Implement Semantic Retrieval Function\n",
        "def retrieve(query, k=3):\n",
        "    # Create query embedding\n",
        "    query_embedding = client.models.embed_content(\n",
        "        model=embedding_model,\n",
        "        contents=query\n",
        "    ).embeddings[0].values\n",
        "\n",
        "    query_embedding = np.array([query_embedding]).astype(\"float32\")\n",
        "\n",
        "    # Search in FAISS\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    print(\"Retrieved chunk indices:\", indices)\n",
        "\n",
        "    retrieved_texts = [chunks[i].page_content for i in indices[0]]\n",
        "\n",
        "    return retrieved_texts"
      ],
      "metadata": {
        "id": "uzxLw__f8jY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = retrieve(\"What is the objective of the proposed model?\")"
      ],
      "metadata": {
        "id": "wNTxWpGA84tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9:: Implement Retrieval-Augmented Answer Generation\n",
        "def generate_answer(query):\n",
        "    retrieved_chunks = retrieve(query)\n",
        "\n",
        "    context = \"\\n\\n\".join(retrieved_chunks)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Answer the question using ONLY the context below.\n",
        "If the answer is not in the context, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"models/gemini-2.5-flash\",\n",
        "        contents=prompt\n",
        "    )\n",
        "\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "zxCHFp489A98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10:: Execute Full RAG Pipeline with User Question\n",
        "question = \"What are the future works suggested?\"\n",
        "\n",
        "answer = generate_answer(question)\n",
        "\n",
        "print(\"Final Answer:\\n\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "MzEJCYqJ9M2B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}