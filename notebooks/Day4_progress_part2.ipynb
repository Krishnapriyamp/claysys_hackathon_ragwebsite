{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCgWg3fl7dCA"
      },
      "outputs": [],
      "source": [
        "!pip install -U google-genai faiss-cpu beautifulsoup4 requests numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import faiss\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google import genai\n",
        "\n",
        "#  Set your Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key\"\n",
        "\n",
        "client = genai.Client()"
      ],
      "metadata": {
        "id": "nPg9iuS17jOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step1::Load Website Content\n",
        "def load_website(url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Remove unwanted elements\n",
        "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\"]):\n",
        "        tag.decompose()\n",
        "\n",
        "    paragraphs = soup.find_all(\"p\")\n",
        "    text = \" \".join([p.get_text() for p in paragraphs])\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "C-_DyWnQ7mgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step2::Text Chunking\n",
        "def chunk_text(text, chunk_size=400, overlap=80):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(words):\n",
        "        end = start + chunk_size\n",
        "        chunk = \" \".join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "a3mXJ5y07peS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_text(text):\n",
        "    response = client.models.embed_content(\n",
        "        model=\"models/gemini-embedding-001\",\n",
        "        contents=text\n",
        "    )\n",
        "    return np.array(response.embeddings[0].values, dtype=\"float32\")"
      ],
      "metadata": {
        "id": "tueJYe98G6Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step3::Create Documents with Cached Embeddings\n",
        "url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
        "\n",
        "website_text = load_website(url)\n",
        "chunks = chunk_text(website_text)\n",
        "\n",
        "\n",
        "documents = []\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    emb = embed_text(chunk)   # Generate embedding once\n",
        "\n",
        "    documents.append({\n",
        "        \"text\": chunk,\n",
        "        \"source\": f\"{url} | Chunk {i+1}\",\n",
        "        \"embedding\": emb\n",
        "    })\n",
        "\n",
        "print(\"Total Chunks:\", len(documents))"
      ],
      "metadata": {
        "id": "wXP6KCDS7s1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step5:: Build FAISS Vector Index\n",
        "embeddings = [doc[\"embedding\"] for doc in documents]\n",
        "dimension = embeddings[0].shape[0]\n",
        "\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(embeddings))\n",
        "\n",
        "print(\"FAISS index built successfully.\")"
      ],
      "metadata": {
        "id": "8LMUBnyy70R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "#  Helper Functions for Advanced RAG\n",
        "# --------------------------------------------\n",
        "# Contains:\n",
        "# - Embedding generation\n",
        "# - Retrieval logic\n",
        "# - Guardrail filtering\n",
        "# - Cosine re-ranking\n",
        "# - Multi-query expansion\n",
        "# - Verification\n",
        "# - Confidence estimation\n",
        "# - Final answer generation\n",
        "# ============================================"
      ],
      "metadata": {
        "id": "kfndYKRZS9vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, top_k=8):\n",
        "    query_vector = embed_text(query)\n",
        "    distances, indices = index.search(np.array([query_vector]), top_k)\n",
        "\n",
        "    results = []\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        results.append({\n",
        "            \"text\": documents[idx][\"text\"],\n",
        "            \"source\": documents[idx][\"source\"],\n",
        "            \"score\": float(dist),\n",
        "            \"embedding\": documents[idx][\"embedding\"]\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "UiEofe1l77NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SIMILARITY_THRESHOLD = 1.2\n",
        "\n",
        "def guardrail_filter(results):\n",
        "    return [r for r in results if r[\"score\"] < SIMILARITY_THRESHOLD]"
      ],
      "metadata": {
        "id": "Czjx3eiI7-xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rerank(query, retrieved_chunks):\n",
        "    chunk_texts = \"\\n\\n\".join(\n",
        "        [f\"Chunk {i+1}: {chunk['text']}\" for i, chunk in enumerate(retrieved_chunks)]\n",
        "    )\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Query: {query}\n",
        "\n",
        "    Rank the top 3 most relevant chunks by number.\n",
        "\n",
        "    {chunk_texts}\n",
        "\n",
        "    Return only numbers separated by commas.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"models/gemini-2.5-flash\",\n",
        "        contents=prompt\n",
        "    )\n",
        "\n",
        "    ranked_numbers = response.text.strip()\n",
        "    top_indices = [int(n.strip()) - 1 for n in ranked_numbers.split(\",\") if n.strip().isdigit()]\n",
        "\n",
        "    return [retrieved_chunks[i] for i in top_indices if i < len(retrieved_chunks)]"
      ],
      "metadata": {
        "id": "LW55Ahxy8pr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_rerank(query, retrieved_chunks, top_k=3):\n",
        "    query_vec = embed_text(query)\n",
        "\n",
        "    reranked = []\n",
        "\n",
        "    for chunk in retrieved_chunks:\n",
        "        chunk_vec = chunk[\"embedding\"]\n",
        "\n",
        "        cosine_sim = np.dot(query_vec, chunk_vec) / (\n",
        "            np.linalg.norm(query_vec) * np.linalg.norm(chunk_vec)\n",
        "        )\n",
        "\n",
        "        chunk[\"cosine_score\"] = float(cosine_sim)\n",
        "        reranked.append(chunk)\n",
        "\n",
        "    reranked = sorted(reranked, key=lambda x: x[\"cosine_score\"], reverse=True)\n",
        "\n",
        "    return reranked[:top_k]"
      ],
      "metadata": {
        "id": "y_wVnYD28usJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sources(chunks):\n",
        "    return list(set(chunk[\"source\"] for chunk in chunks))"
      ],
      "metadata": {
        "id": "vTtZshOO86MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_query(query):\n",
        "    prompt = f\"\"\"\n",
        "    Generate 3 different rephrasings of the following query.\n",
        "\n",
        "    Query: {query}\n",
        "\n",
        "    Return each variation on a new line.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"models/gemini-2.5-flash\",\n",
        "        contents=prompt\n",
        "    )\n",
        "\n",
        "    variations = response.text.strip().split(\"\\n\")\n",
        "    variations = [v.strip(\"- \").strip() for v in variations if v.strip()]\n",
        "\n",
        "    return [query] + variations"
      ],
      "metadata": {
        "id": "qp8fmpcZ9SEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_query_retrieve(query, top_k=5):\n",
        "    queries = expand_query(query)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for q in queries:\n",
        "        results = retrieve(q, top_k)\n",
        "        all_results.extend(results)\n",
        "\n",
        "    # Remove duplicates using source\n",
        "    unique = {r[\"source\"]: r for r in all_results}\n",
        "\n",
        "    return list(unique.values())"
      ],
      "metadata": {
        "id": "WGHURA5C9WQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_answer(answer, context):\n",
        "    prompt = f\"\"\"\n",
        "    Based only on the context below, verify whether the answer is fully supported.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Answer:\n",
        "    {answer}\n",
        "\n",
        "    Reply with only YES or NO.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"models/gemini-2.5-flash\",\n",
        "        contents=prompt\n",
        "    )\n",
        "\n",
        "    return response.text.strip().upper()"
      ],
      "metadata": {
        "id": "5mYpehKx-1mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_confidence(reranked_chunks, verification=\"YES\"):\n",
        "    if not reranked_chunks:\n",
        "        return 0.0\n",
        "\n",
        "    cosine_scores = [chunk[\"cosine_score\"] for chunk in reranked_chunks]\n",
        "    avg_score = np.mean(cosine_scores)\n",
        "\n",
        "    # Normalize cosine (0 to 1 range assumption for meaningful results)\n",
        "    retrieval_conf = max(0, min(1, avg_score))\n",
        "\n",
        "    # Penalize if few chunks support answer\n",
        "    coverage_factor = len(reranked_chunks) / 3  # assuming top_k=3\n",
        "    coverage_factor = min(1, coverage_factor)\n",
        "\n",
        "    confidence = retrieval_conf * coverage_factor\n",
        "\n",
        "    # Penalize if verification fails\n",
        "    if verification != \"YES\":\n",
        "        confidence *= 0.6\n",
        "\n",
        "    return round(confidence, 3)"
      ],
      "metadata": {
        "id": "uUCy4bHf-2go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query, conversation_history):\n",
        "\n",
        "    # Step 1: Retrieve\n",
        "    retrieved = multi_query_retrieve(query)\n",
        "\n",
        "    # Step 2: Guardrail Filter\n",
        "    filtered = guardrail_filter(retrieved)\n",
        "\n",
        "    if not filtered:\n",
        "        return {\n",
        "            \"answer\": \"The answer is not available in the provided website content.\",\n",
        "            \"sources\": [],\n",
        "            \"confidence\": 0.0\n",
        "        }\n",
        "\n",
        "    # Step 3: Cosine Re-rank\n",
        "    reranked = cosine_rerank(query, filtered)\n",
        "\n",
        "    # Step 4: Prepare Context\n",
        "    context = \"\\n\\n\".join([chunk[\"text\"] for chunk in reranked])\n",
        "\n",
        "    # Step 5: Source Attribution\n",
        "    sources = extract_sources(reranked)\n",
        "\n",
        "    # Step 6: Final Answer Generation\n",
        "    history_text = \"\\n\".join(\n",
        "    [f\"User: {h['user']}\\nAssistant: {h['assistant']}\" for h in conversation_history[-3:]]\n",
        "    )\n",
        "\n",
        "    final_prompt = f\"\"\"\n",
        "    You are a helpful assistant.\n",
        "\n",
        "    Conversation History:\n",
        "    {history_text}\n",
        "\n",
        "    Use the context below to answer the question.\n",
        "    If the answer is not found, say it is not available.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Current Question:\n",
        "    {query}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "      model=\"models/gemini-2.5-flash\",\n",
        "      contents=final_prompt\n",
        "    )\n",
        "\n",
        "    answer = response.text\n",
        "\n",
        "    confidence = calculate_confidence(reranked)\n",
        "\n",
        "    # Verification step\n",
        "    verification = verify_answer(answer, context)\n",
        "\n",
        "    if verification != \"YES\":\n",
        "        confidence *= 0.6   # reduce confidence if not fully supported\n",
        "\n",
        "    conversation_history.append({\n",
        "    \"user\": query,\n",
        "    \"assistant\": answer\n",
        "    })\n",
        "\n",
        "    return {\n",
        "      \"answer\": answer,\n",
        "      \"sources\": sources,\n",
        "      \"confidence\": round(confidence, 3),\n",
        "      \"verified\": verification\n",
        "    }"
      ],
      "metadata": {
        "id": "_tHVDumx9iGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "#  Interactive Chat Loop with Memory\n",
        "# --------------------------------------------\n",
        "# - Maintain short-term conversation history\n",
        "# - Allow user to ask multiple questions\n",
        "# - Support follow-up queries\n",
        "# - Exit when user types 'exit'\n",
        "# ============================================"
      ],
      "metadata": {
        "id": "gJS7VV_tTKpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history = []\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nAsk a question (type 'exit' to stop): \")\n",
        "\n",
        "    if query.lower() == \"exit\":\n",
        "        print(\"Exiting chatbot.\")\n",
        "        break\n",
        "\n",
        "    result = generate_answer(query, conversation_history)\n",
        "\n",
        "    print(\"\\nAnswer:\\n\")\n",
        "    print(result[\"answer\"])\n",
        "\n",
        "    print(\"\\nSources:\")\n",
        "    for s in result[\"sources\"]:\n",
        "        print(\"-\", s)\n",
        "\n",
        "    print(\"\\nConfidence Score:\", result[\"confidence\"])\n",
        "    print(\"Verified:\", result[\"verified\"])"
      ],
      "metadata": {
        "id": "1FOqcJ3B-ffN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}